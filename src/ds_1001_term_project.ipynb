{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "DEFAULT_DATA_DIR = \"../data/globalterrorismdb_0616dist.txt\"\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv(DEFAULT_DATA_DIR, sep=\"\\t\", na_values=['.', 'nan'], low_memory=False, encoding=\"ISO-8859-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156772\n",
      "137\n",
      "3290\n",
      "106\n",
      "79\n",
      "58\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "### EXPLORTARY ANALYSIS ###\n",
    "#print(len(data))\n",
    "#print(len(data.columns))\n",
    "#for col in data.columns:\n",
    "#    print(\"{0}, type={3}, {1}% null, {2} uniqe val\".format(col, float(np.sum(pd.isnull(data[col])))/len(data)*100, len(np.unique(data[col]))\n",
    "#, data[col].dtype))\n",
    "\n",
    "print(len(data))\n",
    "print(len(data.columns))\n",
    "print(len(np.unique(data['gname'])))\n",
    "has_null = 0\n",
    "more_than_50_null = 0\n",
    "text_col = 0\n",
    "more_than_30_null_or_text = 0\n",
    "for col in data.columns:\n",
    "    if float(np.sum(pd.isnull(data[col])))/len(data) > 0:\n",
    "        has_null += 1\n",
    "        if float(np.sum(pd.isnull(data[col])))/len(data) > 0.5:\n",
    "            more_than_50_null += 1\n",
    "    if str(data[col].dtype) == \"object\":\n",
    "        text_col +=1\n",
    "    if float(np.sum(pd.isnull(data[col])))/len(data) > 0.3 or str(data[col].dtype) == \"object\":\n",
    "        more_than_30_null_or_text += 1\n",
    "print(has_null)\n",
    "print(more_than_50_null)\n",
    "print(text_col)\n",
    "print(more_than_30_null_or_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, u'01/04/2000', u'01/17/2006', ..., u'September 8-14, 2013',\n",
       "       u'Within the first 10 days of the month', u'Yes'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data['approxdate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Initial Explortary Analysis Summary:\n",
    "\n",
    "156772 ROWS\n",
    "\n",
    "137 COLUMNS\n",
    "\n",
    "Original Columns:\n",
    "\n",
    "- Time\n",
    " - iyear, type=int64, 0.0% null, 45 uniqe val\n",
    " - imonth, type=int64, 0.0% null, 13 uniqe val\n",
    " - iday, type=int64, 0.0% null, 32 uniqe val\n",
    " - approxdate, type=object, 96.9662950017% null, 1427 uniqe val\n",
    " - extended, type=int64, 0.0% null, 2 uniqe val\n",
    " - resolution, type=object, 97.7661827367% null, 2658 uniqe val\n",
    "\n",
    "Notes:\n",
    "  - Some of of the $imonth$ and $iday$ are 0, which means that the exact date is unknown. Estimates are given in $approxdate$, but the format in $approxdate$ varies.\n",
    "\n",
    "  - Need to extract the week day out of the date feature.\n",
    "\n",
    "  - $resolution$ is not meaningful as most of them is null. $approxdate$ can be dropped once it's used to impute date.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Location\n",
    " - country, type=int64, 0.0% null, 206 uniqe val\n",
    " - country_txt, type=object, 0.0% null, 206 uniqe val\n",
    " - region, type=int64, 0.0% null, 12 uniqe val\n",
    " - region_txt, type=object, 0.0% null, 12 uniqe val\n",
    " - provstate, type=object, 9.26185798484% null, 2510 uniqe val\n",
    " - city, type=object, 0.28448957722% null, 31325 uniqe val\n",
    " - latitude, type=float64, 2.88253004363% null, 56540 uniqe val\n",
    " - longitude, type=float64, 2.88253004363% null, 56151 uniqe val\n",
    " - specificity, type=int64, 0.0% null, 5 uniqe val\n",
    " - vicinity, type=int64, 0.0% null, 3 uniqe val\n",
    " - location, type=object, 73.0749113362% null, 35798 uniqe val\n",
    " \n",
    "Notes:\n",
    "  - $contry$ and $region$ should stay but their txt counter part should be dropped.\n",
    "  - $latitude$ and $longitude$ have some problem. Including them would significantly increase the model training time. Need to find out why! TODO.\n",
    "  - $specificity$ and $vicinity$ can stay and they are in good quality\n",
    "  - $location$ will likely be dropped. Too many nulls. Maybe can try to extract some info using NLP.\n",
    "  - $provstate$ and $city$ are in question. They are too many unique vals and info may be repeated in other columns.\n",
    "  \n",
    "\n",
    "\n",
    "- Criterion\n",
    " - crit1, type=int64, 0.0% null, 2 uniqe val\n",
    " - crit2, type=int64, 0.0% null, 2 uniqe val\n",
    " - crit3, type=int64, 0.0% null, 2 uniqe val\n",
    " - doubtterr, type=float64, 0.000637869007221% null, 4 uniqe val\n",
    " - alternative, type=float64, 84.540606741% null, 132541 uniqe val\n",
    " - alternative_txt, type=object, 84.540606741% null, 6 uniqe val\n",
    "\n",
    "Notes:\n",
    "  - $crit1$, $crit2$, and $crit3$ are useful and in good quality.\n",
    "  - Others are either too much null or not useful.\n",
    "\n",
    "\n",
    "\n",
    "- Event \n",
    " - eventid, type=int64, 0.0% null, 156772 uniqe val\n",
    " - summary, type=object, 42.1886561376% null, 88704 uniqe val\n",
    " - multiple, type=int64, 0.0% null, 2 uniqe val\n",
    " - success, type=int64, 0.0% null, 2 uniqe val\n",
    " - suicide, type=int64, 0.0% null, 2 uniqe val\n",
    " - motive, type=object, 70.2804072156% null, 11683 uniqe val\n",
    " - nperps, type=float64, 45.3728982216% null, 71245 uniqe val\n",
    " - nperpcap, type=float64, 44.3382746919% null, 69560 uniqe val\n",
    " - claimed, type=float64, 42.1892940066% null, 66145 uniqe val\n",
    " - claimmode, type=float64, 91.5316510601% null, 143507 uniqe val\n",
    " - claimmode_txt, type=object, 91.5316510601% null, 12 uniqe val\n",
    " - claim2, type=float64, 99.3015334371% null, 155680 uniqe val\n",
    " - claimmode2, type=float64, 99.7416630521% null, 156376 uniqe val\n",
    " - claimmode2_txt, type=object, 99.7416630521% null, 10 uniqe val\n",
    " - claim3, type=float64, 99.8992166969% null, 156616 uniqe val\n",
    " - claimmode3, type=float64, 99.9515219555% null, 156704 uniqe val\n",
    " - claimmode3_txt, type=object, 99.9515219555% null, 9 uniqe val\n",
    " - compclaim, type=float64, 97.0600617457% null, 152166 uniqe val\n",
    " \n",
    "Notes:\n",
    "  - $eventid$ is useless.\n",
    "  - $summary$ is in English and need to be NLP-processed\n",
    "  - $multiple$, $success$, and $suicide$ are useful and in good quality. \n",
    "\n",
    "\n",
    "- Attack\n",
    " - attacktype1, type=int64, 0.0% null, 9 uniqe val\n",
    " - attacktype1_txt, type=object, 0.0% null, 9 uniqe val\n",
    " - attacktype2, type=float64, 96.8323425101% null, 151815 uniqe val\n",
    " - attacktype2_txt, type=object, 96.8323425101% null, 10 uniqe val\n",
    " - attacktype3, type=float64, 99.8009848697% null, 156468 uniqe val\n",
    " - attacktype3_txt, type=object, 99.8009848697% null, 9 uniqe val\n",
    "\n",
    "Notes:\n",
    "  - Only $attacktype1$ should stay.\n",
    "\n",
    "\n",
    "- Target\n",
    " - targtype1, type=int64, 0.0% null, 22 uniqe val\n",
    " - targtype1_txt, type=object, 0.0% null, 22 uniqe val\n",
    " - targsubtype1, type=float64, 5.20182175388% null, 8265 uniqe val\n",
    " - targsubtype1_txt, type=object, 5.20182175388% null, 111 uniqe val\n",
    " - corp1, type=object, 27.1579108514% null, 29296 uniqe val\n",
    " - target1, type=object, 0.413339116679% null, 79948 uniqe val\n",
    " - natlty1, type=float64, 0.783303140867% null, 1440 uniqe val\n",
    " - natlty1_txt, type=object, 0.783303140867% null, 213 uniqe val\n",
    " - targtype2, type=float64, 94.3172250147% null, 147885 uniqe val\n",
    " - targtype2_txt, type=object, 94.3172250147% null, 23 uniqe val\n",
    " - targsubtype2, type=float64, 94.5978873778% null, 148406 uniqe val\n",
    " - targsubtype2_txt, type=object, 94.5978873778% null, 104 uniqe val\n",
    " - corp2, type=object, 94.9735922231% null, 2345 uniqe val\n",
    " - target2, type=object, 94.3982343786% null, 4562 uniqe val\n",
    " - natlty2, type=float64, 94.5143265379% null, 148326 uniqe val\n",
    " - natlty2_txt, type=object, 94.5143265379% null, 155 uniqe val\n",
    " - targtype3, type=float64, 99.4055060853% null, 155860 uniqe val\n",
    " - targtype3_txt, type=object, 99.4055060853% null, 21 uniqe val\n",
    " - targsubtype3, type=float64, 99.4514326538% null, 155997 uniqe val\n",
    " - targsubtype3_txt, type=object, 99.4514326538% null, 86 uniqe val\n",
    " - corp3, type=object, 99.5011864364% null, 352 uniqe val\n",
    " - target3, type=object, 99.4067818233% null, 631 uniqe val\n",
    " - natlty3, type=float64, 99.4233664175% null, 155969 uniqe val\n",
    " - natlty3_txt, type=object, 99.4233664175% null, 102 uniqe val\n",
    " \n",
    " Notes:\n",
    "   - $targtype1$ statys, $natlty1$ statys but with null rows dropped\n",
    "   - $targsubtype1$, $target1$ may stay\n",
    "   - all other dropped, > 90% null\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "- Perpetrator\n",
    " - gname, type=object, 0.0% null, 3290 uniqe val\n",
    " - gsubname, type=object, 97.0179623912% null, 993 uniqe val\n",
    " - gname2, type=object, 99.2224376802% null, 335 uniqe val\n",
    " - gsubname2, type=object, 99.9228178501% null, 44 uniqe val\n",
    " - gname3, type=object, 99.8953894828% null, 78 uniqe val\n",
    " - ingroup, type=int64, 0.0% null, 3290 uniqe val\n",
    " - ingroup2, type=float64, 99.2224376802% null, 155887 uniqe val\n",
    " - ingroup3, type=float64, 99.8953894828% null, 156685 uniqe val\n",
    " - gsubname3, type=object, 99.996172786% null, 7 uniqe val\n",
    " - guncertain1, type=float64, 0.241114484729% null, 380 uniqe val\n",
    " - guncertain2, type=float64, 99.2587962136% null, 155612 uniqe val\n",
    " - guncertain3, type=float64, 99.8979409588% null, 156614 uniqe val\n",
    " \n",
    "Notes:\n",
    "  - Only $gname$ should stay and encoded.\n",
    "\n",
    "\n",
    "\n",
    "- Weapon\n",
    " - weaptype1, type=int64, 0.0% null, 12 uniqe val\n",
    " - weaptype1_txt, type=object, 0.0% null, 12 uniqe val\n",
    " - weapsubtype1, type=float64, 11.4223203123% null, 17935 uniqe val\n",
    " - weapsubtype1_txt, type=object, 11.4223203123% null, 29 uniqe val\n",
    " - weaptype2, type=float64, 93.3578700278% null, 146370 uniqe val\n",
    " - weaptype2_txt, type=object, 93.3578700278% null, 12 uniqe val\n",
    " - weapsubtype2, type=float64, 94.0710075779% null, 147503 uniqe val\n",
    " - weapsubtype2_txt, type=object, 94.0710075779% null, 27 uniqe val\n",
    " - weaptype3, type=float64, 99.0865715817% null, 155350 uniqe val\n",
    " - weaptype3_txt, type=object, 99.0865715817% null, 11 uniqe val\n",
    " - weapsubtype3, type=float64, 99.1669430766% null, 155488 uniqe val\n",
    " - weapsubtype3_txt, type=object, 99.1669430766% null, 23 uniqe val\n",
    " - weaptype4, type=float64, 99.9527976935% null, 156703 uniqe val\n",
    " - weaptype4_txt, type=object, 99.9527976935% null, 6 uniqe val\n",
    " - weapsubtype4, type=float64, 99.9547113005% null, 156717 uniqe val\n",
    " - weapsubtype4_txt, type=object, 99.9547113005% null, 17 uniqe val\n",
    " - weapdetail, type=object, 32.4917714898% null, 16988 uniqe val\n",
    "\n",
    "Notes:\n",
    "  - $weaptype1$ should stay.\n",
    "  - $weapsubtype1$ may stay, $weapdetail$ may stay after NLP processing.\n",
    "\n",
    "- Consequence\n",
    " - nkill, type=float64, 5.70573826959% null, 9283 uniqe val\n",
    " - nkillus, type=float64, 41.1151225984% null, 64487 uniqe val\n",
    " - nkillter, type=float64, 42.3468476514% null, 66521 uniqe val\n",
    " - nwound, type=float64, 9.05773990253% null, 14576 uniqe val\n",
    " - nwoundus, type=float64, 41.2739519812% null, 64749 uniqe val\n",
    " - nwoundte, type=float64, 43.5728318832% null, 68374 uniqe val\n",
    " - property, type=int64, 0.0% null, 3 uniqe val\n",
    " - propextent, type=float64, 64.0548057051% null, 100424 uniqe val\n",
    " - propextent_txt, type=object, 64.0548057051% null, 5 uniqe val\n",
    " - propvalue, type=float64, 80.0270456459% null, 126064 uniqe val\n",
    " - propcomment, type=object, 68.4758757941% null, 17459 uniqe val\n",
    " - ishostkid, type=float64, 0.113540683285% null, 181 uniqe val\n",
    " - nhostkid, type=float64, 92.8124920266% null, 145725 uniqe val\n",
    " - nhostkidus, type=float64, 92.847574822% null, 145587 uniqe val\n",
    " - nhours, type=float64, 97.8937565382% null, 153504 uniqe val\n",
    " - ndays, type=float64, 95.8015461945% null, 150479 uniqe val\n",
    " - divert, type=object, 99.8156558569% null, 143 uniqe val\n",
    " - kidhijcountry, type=object, 97.9014109662% null, 218 uniqe val\n",
    " - ransom, type=float64, 52.1011405098% null, 81683 uniqe val\n",
    " - ransomamt, type=float64, 99.2377465364% null, 155927 uniqe val\n",
    " - ransomamtus, type=float64, 99.737835838% null, 156382 uniqe val\n",
    " - ransompaid, type=float64, 99.6026076085% null, 156271 uniqe val\n",
    " - ransompaidus, type=float64, 99.7435766591% null, 156378 uniqe val\n",
    " - ransomnote, type=object, 99.731457148% null, 297 uniqe val\n",
    " - hostkidoutcome, type=float64, 94.4601076723% null, 148094 uniqe val\n",
    " - hostkidoutcome_txt, type=object, 94.4601076723% null, 8 uniqe val\n",
    " - nreleased, type=float64, 94.8364503865% null, 148832 uniqe val\n",
    " - addnotes, type=object, 86.0153598857% null, 12762 uniqe val\n",
    " \n",
    "Notes:\n",
    "  - $nkill$, $nwound$ staty will null imputed as avg.\n",
    "  - $ishostkid$, $property$ stay, drop NAï¼Œ deal with unknown\n",
    "\n",
    "\n",
    "- Reference\n",
    " - scite1, type=object, 42.3098512489% null, 66823 uniqe val\n",
    " - scite2, type=object, 60.9872936494% null, 50240 uniqe val\n",
    " - scite3, type=object, 78.2282550455% null, 28555 uniqe val\n",
    " - dbsource, type=object, 0.0% null, 26 uniqe val\n",
    " - INT_LOG, type=int64, 0.0% null, 3 uniqe val\n",
    " - INT_IDEO, type=int64, 0.0% null, 3 uniqe val\n",
    " - INT_MISC, type=int64, 0.0% null, 3 uniqe val\n",
    " - INT_ANY, type=int64, 0.0% null, 3 uniqe val\n",
    " - related, type=object, 86.9734391345% null, 20030 uniqe val\n",
    " \n",
    "Notes:\n",
    "  - All columns above are reference columns and should be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Data Processing Functions ####\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "\n",
    "def clean_dataset(dataset, drop_unknown=True):\n",
    "    \"\"\"\n",
    "    Function that cleans up the data set\n",
    "    - select relevant columns\n",
    "    - clean up NA / bad values\n",
    "    - drop bad rows\n",
    "    \"\"\"\n",
    "    # select useful columns\n",
    "    col_need = [\"iyear\", \"imonth\", \"iday\", \"extended\",\n",
    "                \"country\", \"region\", \"vicinity\", \"specificity\",\n",
    "                \"crit1\", \"crit2\", \"crit3\",\n",
    "                \"multiple\", \"success\", \"suicide\",\n",
    "                \"attacktype1\",\n",
    "                \"targtype1\", \"targsubtype1\", \"natlty1\",\n",
    "                \"gname\",\n",
    "                \"weaptype1\", \"weapsubtype1\",\n",
    "                \"nkill\", \"nwound\", \"ishostkid\", \"property\"]\n",
    "    \n",
    "    clean_ds = dataset[col_need]\n",
    "    # clean up NA\n",
    "    # columns to drop if NA\n",
    "    drop_na_cols = [\"gname\", \"natlty1\", \"ishostkid\", \"property\",\n",
    "                   \"targsubtype1\", \"weapsubtype1\"]\n",
    "    old_len = len(clean_ds)\n",
    "    clean_ds = clean_ds.dropna(subset=drop_na_cols)\n",
    "    new_len = len(clean_ds)\n",
    "    print(\"{0} out of {1}({2}%) rows were dropped because of NA in {3}\".format(old_len-new_len, old_len, \n",
    "                                                                               float(old_len-new_len)/old_len*100,\n",
    "                                                                              str(drop_na_cols)))\n",
    "    # columns to drop if unknown\n",
    "    if drop_unknown:\n",
    "        col_to_unknown_map = {\"gname\":\"Unknown\", \"imonth\":0, \"iday\":0}\n",
    "        for col in col_to_unknown_map:\n",
    "            clean_ds = clean_ds[clean_ds[col]!=col_to_unknown_map[col]]\n",
    "        new_len = len(clean_ds)\n",
    "        print(\"{0} out of {1}({2}%) rows were dropped because of unknown values in {3}\".format(old_len-new_len, old_len, \n",
    "                                                                                   float(old_len-new_len)/old_len*100,\n",
    "                                                                                  str(col_to_unknown_map.keys())))    \n",
    "    \n",
    "    # columns to impute NA using average\n",
    "    fill_na_dict = {\"nkill\":0, \"nwound\":0}\n",
    "    for col in fill_na_dict: \n",
    "        clean_ds[col]= clean_ds[col].fillna(fill_na_dict[col])\n",
    "    return clean_ds\n",
    "\n",
    "def validate_col(col):\n",
    "    na_flag = pd.isnull(col)\n",
    "    return na_flag \n",
    "\n",
    "def validate_df(dataset):\n",
    "    \"\"\"\n",
    "    Function that validates a dataset\n",
    "    \"\"\"\n",
    "    # check NA\n",
    "    for col in dataset.columns:\n",
    "        na_num = np.sum(validate_col(dataset[col]))\n",
    "        if na_num > 0:\n",
    "            print(\"Warning: {0} rows ({1}) in column {2} is NA.\".format(na_num, float(na_num)/len(dataset),col))\n",
    "    \n",
    "\n",
    "\n",
    "def extract_features(dataset, top_ten=False, encode_cat=False):\n",
    "    feature_df = dataset #dataset[min_set_feature]\n",
    "    \n",
    "    # Time Related Feature\n",
    "    # TODO: try to impute time with imonth/iday = 0 using approxdate\n",
    "    dataset[\"weekday\"] = dataset.apply(lambda x: date(x.iyear, x.imonth, x.iday).weekday(),1)\n",
    "    \n",
    "    # Encode categorical features\n",
    "    if encode_cat:\n",
    "        categorical_columns = [\"imonth\",\"iday\", \"weekday\",\"country\", \"attacktype1\", \"targtype1\", \"targsubtype1\", \"natlty1\", \"weaptype1\", \"weapsubtype1\"]\n",
    "        for cat_col in categorical_columns:\n",
    "            all_unique_val = np.unique(dataset[cat_col])\n",
    "            for val in all_unique_val:\n",
    "                dataset[\"{0}={1}\".format(cat_col,val)] = dataset.apply(lambda x: x[cat_col]==val,1)\n",
    "            dataset = dataset.drop(cat_col,1)\n",
    "    \n",
    "    # Encode Label\n",
    "    if top_ten:\n",
    "        top_ten_gname = feature_df.groupby(\"gname\").apply(lambda x: pd.Series({\"count\":len(x)})).sort_values(\"count\", ascending=False).head(10).to_dict()\n",
    "        top_ten = top_ten_gname['count'].keys()\n",
    "        top_ten_dict=dict([(top_ten[i],i) for i in range(len(list(top_ten)))])\n",
    "        feature_df['Y'] = feature_df.apply(lambda x: 10 if x['gname'] not in top_ten_dict else top_ten_dict[x['gname']],1)\n",
    "    else:\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(feature_df['gname'])\n",
    "        feature_df['Y'] = label_encoder.transform(feature_df['gname'])\n",
    "    encoded_df = feature_df.drop(['gname'],1)\n",
    "\n",
    "    \n",
    "    # validate dataset\n",
    "    validate_df(encoded_df)\n",
    "        \n",
    "    return encoded_df\n",
    "\n",
    "def prepare_train_test_set(dataset, label=\"Y\", test_ratio=0.2, time_logic=True):\n",
    "    \"\"\"\n",
    "    Takes a post-feature-engineered dataset.\n",
    "    @param time_logic: if True, the test samples will be the train_ratio% latest data \n",
    "    Return train_x, train_y, test_x, test_y\n",
    "    \"\"\"\n",
    "    dataset = dataset.sort_values(\"iyear\").reset_index().drop(\"index\",1)\n",
    "    drop_cols = [label, 'iyear'] if time_logic else [label]\n",
    "    x = dataset.drop(drop_cols, 1).as_matrix()        \n",
    "    y = dataset[label].as_matrix()\n",
    "    if not time_logic:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_ratio)\n",
    "    else:\n",
    "        threshold = int(round(len(x)*(1-test_ratio)))\n",
    "        x_train = x[:threshold]\n",
    "        y_train = y[:threshold]\n",
    "        x_test = x[threshold:]\n",
    "        y_test = y[threshold:]\n",
    "    return [x_train, x_test, y_train, y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25663 out of 156772(16.3696323323%) rows were dropped because of NA in ['gname', 'natlty1', 'ishostkid', 'property', 'targsubtype1', 'weapsubtype1']\n",
      "87504 out of 156772(55.8160896078%) rows were dropped because of unknown values in ['iday', 'imonth', 'gname']\n",
      "[(55414, 24), (13854, 24), (55414,), (13854,)]\n",
      "iyear           1970\n",
      "imonth             1\n",
      "iday               1\n",
      "extended           0\n",
      "country          217\n",
      "region             1\n",
      "vicinity           0\n",
      "specificity        1\n",
      "crit1              1\n",
      "crit2              1\n",
      "crit3              1\n",
      "multiple           0\n",
      "success            1\n",
      "suicide            0\n",
      "attacktype1        2\n",
      "targtype1          3\n",
      "targsubtype1      22\n",
      "natlty1          217\n",
      "weaptype1          5\n",
      "weapsubtype1       5\n",
      "nkill              0\n",
      "nwound             0\n",
      "ishostkid          0\n",
      "property           1\n",
      "weekday            3\n",
      "Y                505\n",
      "Name: 5, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Clean Dataset\n",
    "clean_data = clean_dataset(data)\n",
    "feature_data = extract_features(clean_data)\n",
    "x_train, x_test, y_train, y_test = prepare_train_test_set(feature_data)\n",
    "print([x_train.shape, x_test.shape, y_train.shape, y_test.shape])\n",
    "print(feature_data.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#feature_data.to_csv(\"../data/feature_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualization #\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def visualize_distribution(data, col, na_filler=\"na\", xlim=None, top_k=None, categorical=False):\n",
    "    \"\"\"\n",
    "    Plot a histogram for the distribution on col\n",
    "    @param categorical: if the column is categorical\n",
    "    @param top_k: only show top k group, ONLY VALID FOR NON-NUMERIC\n",
    "    \"\"\"\n",
    "    # replace NaN\n",
    "    data[col] = data[col].fillna(na_filler)\n",
    "    if categorical:\n",
    "        # if top k\n",
    "        if top_k is not None:\n",
    "            data = data[[col]]\n",
    "            top_k_list = data.groupby(col).apply(lambda x: pd.Series({\"count\":len(x)})).sort_values(\"count\", ascending=False).head(top_k)\n",
    "            top_k_list = top_k_list.to_dict()[\"count\"].keys()\n",
    "            data[col] = data.apply(lambda x: \"other\" if x[col] not in top_k_list else x[col],1)\n",
    "        data[col].value_counts().plot(kind='bar')\n",
    "    else:\n",
    "        plot_data = data[col]\n",
    "        # the histogram of the data\n",
    "        if xlim is not None:\n",
    "            n, bins, patches = plt.hist(plot_data, normed=1, range=xlim, facecolor='green', alpha=0.75)\n",
    "            plt.xlim(xlim)\n",
    "        else:\n",
    "            n, bins, patches = plt.hist(plot_data, normed=1, facecolor='green', alpha=0.75)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frequency %')\n",
    "        plt.title(\"Distribution of {0}\".format(col))\n",
    "        plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_learning_curve_multiple(data, x_val, ylabel=\"ACC\"):\n",
    "    \"\"\"\n",
    "    Function that plots multiple lines with mean & std\n",
    "    @param data: data[model_name][x_val]=[all data]\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(\"Learning Curve - Model Comparison\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    for model_name in data:\n",
    "        y_mean = []\n",
    "        y_std = []\n",
    "        for x in x_val:\n",
    "            y_mean.append(np.mean(data[model_name][x]))\n",
    "            y_std.append(np.std(data[model_name][x]))\n",
    "        plt.errorbar(x_val, y_mean, yerr=y_std, label=model_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5),\n",
    "                       metrics=accuracy_score):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, scoring=make_scorer(metrics), cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "def cross_validation_plot():\n",
    "    \"\"\"\n",
    "    Function that shows plot for tunning\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# feature engineering\n",
    "# terrorist group analysis\n",
    "#print clean_data.groupby(\"gname\").apply(lambda x: pd.Series({\"count\":len(x)})).sort_values(\"count\", ascending=False).head(10)\n",
    "\n",
    "# print(np.unique(data[\"nkillus\"]))\n",
    "# visualize_distribution(clean_data, \"gname\", top_k=10, categorical=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Eval Block\n",
    "import numpy as np\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def quick_test_model(x_train, x_test, y_train, y_test, model, eval_metrics):\n",
    "    \"\"\"\n",
    "    Run a quick test on the model, reports the metrics\n",
    "    \"\"\"\n",
    "    model.fit(x_train, y_train)\n",
    "    pred = model.predict(x_test)\n",
    "    return eval_metrics(y_test,pred)\n",
    "\n",
    "def compute_learning_curve_multiple(model_dict, x_train, y_train, x_test, \n",
    "                                 y_test, metrics, chunks, rep=10):\n",
    "    \"\"\"\n",
    "    Plot out-of-sample learning curve against each models\n",
    "    @param model_dict \" {name -> models}\n",
    "    \"\"\"\n",
    "    # model_performance[model_name][chunk_i]=[]\n",
    "    model_performance = {}\n",
    "    for chunk in chunks:\n",
    "        for i in range(rep):\n",
    "            # shuffle every iteration\n",
    "            train_idx = range(len(x_train))\n",
    "            np.random.shuffle(train_idx)\n",
    "            train_idx = train_idx[:chunk]\n",
    "            current_train_x = x_train[train_idx]\n",
    "            current_train_y = y_train[train_idx]\n",
    "            for model_name in model_dict:\n",
    "                if model_name not in model_performance:\n",
    "                    model_performance[model_name] = {}\n",
    "                if chunk not in model_performance[model_name]:\n",
    "                    model_performance[model_name][chunk]=[]\n",
    "                model_performance[model_name][chunk].append(quick_test_model(current_train_x, x_test, \n",
    "                                                                      current_train_y, y_test, \n",
    "                                                                      model_dict[model_name], metrics))\n",
    "    return model_performance\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# models\n",
    "lr = LogisticRegression()\n",
    "dtree = DecisionTreeClassifier(min_samples_split=40)\n",
    "rf = RandomForestClassifier(min_samples_split=10)\n",
    "\n",
    "\n",
    "# Run Model\n",
    "#dtree.fit(x_train, y_train)\n",
    "#print(quick_test_model(x_train, x_test, y_train, y_test, rf, accuracy_score))\n",
    "#print(quick_test_model(x_train, x_test, y_train, y_test, lr, accuracy_score))\n",
    "#print(quick_test_model(x_train, x_test, y_train, y_test, dtree, accuracy_score))\n",
    "\n",
    "\n",
    "# learning curve\n",
    "#curve_plot = plot_learning_curve(model, \"Leanring Curve - LR\", x_train, y_train)\n",
    "#curve_plot.show()\n",
    "\n",
    "# compare model\n",
    "#x_val = [1000,2000,3000]\n",
    "#model_dict = {\"LR\":lr, \"dt\":dtree}\n",
    "#res = compute_learning_curve_multiple(model_dict, x_train, y_train, x_test, y_test, accuracy_score, x_val)\n",
    "#plot_learning_curve_multiple(res, x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
